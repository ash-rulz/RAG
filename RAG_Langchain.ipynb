{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5UsX7ydTlQFx6uJkOvgZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ash-rulz/RAG/blob/main/RAG_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG based QA using Langchain\n",
        "\n",
        "We are using FLAN_T5 model in this excercise. The predecessor of this model is the T5 model which originated from the paper - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(see [video](https://www.youtube.com/watch?v=91iLu6OOrwk) for more details on the paper, or even detailed video [here](https://www.youtube.com/watch?v=Axo0EtMUK90)). The T5 model is based on the paper Scaling Instruction-Finetuned Language Models(see [video](https://www.youtube.com/watch?v=SHMsdAPo2Ls)). FLAN T5 is just Fine-tuned LANguage model on T5.\n",
        "\n",
        "Next, we use [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) which converts the sentences to a 384 dimensional vector space. This sentence embedding is stored in FAISS vector DB.\n",
        "\n",
        "The relevant documents are retrieved using *RetrievalQA* from *langchain*.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Prerequisites:\n",
        "1. \"data\" folder with the pdf and the evaluation dataset need to be saved.\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "weC8tWvd8fnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites\n",
        "Create a folder called example_data and in that folder place the pdf to be stored in the vector database."
      ],
      "metadata": {
        "id": "_LMqN8d8IZz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Split the PDF"
      ],
      "metadata": {
        "id": "FhfM7KE2K1dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain pypdf"
      ],
      "metadata": {
        "id": "J-Ksi8wm3X63"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the pdf to memory\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "pdfLoader = PyPDFLoader(\"data/LetterToIndustry.pdf\")\n",
        "documents = pdfLoader.load()"
      ],
      "metadata": {
        "id": "FWqT6IM-5JBB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mflrq0mu5vLB",
        "outputId": "10758443-0bfe-4eb2-8cc5-eda979900f81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the file to chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        "    separators=['\\n\\n', '\\n', '(?=>\\. )', ' ', ''])\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "jVmzx9Bd53BB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inNZqPcp6VNP",
        "outputId": "57e18ec0-fabd-425d-f367-0f67d9457de4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2: Create vector store\n",
        "Here we use all-MiniLM-L6-v2 to create the sentence embedding and the embeddings are stored in the [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss) vector store."
      ],
      "metadata": {
        "id": "00Hs7y-iLEnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q sentence-transformers"
      ],
      "metadata": {
        "id": "5RB3TlX87Opv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_path,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "iOugJauJ6cY6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-gpu"
      ],
      "metadata": {
        "id": "27T1vXPx-GOx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create vector store\n",
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "MnEg-wk87XO1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of documents retrieved from the vector DB for a question\n",
        "question = \"How many credits does the thesis comprise of?\"\n",
        "searchDocs = db.similarity_search_with_score(question)\n",
        "print(searchDocs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukQ8eKgn8Dub",
        "outputId": "3bd2f5c3-d692-4307-c022-b4f91d938c6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Document(page_content='• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0}), 0.6723114)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3: Create the generator\n",
        "Here we use the FLAN-T5 model which is fine-tuned on many tasks including QA tasks.\n"
      ],
      "metadata": {
        "id": "ZVsYnr_JMds_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "model_name_flan = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_flan)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_flan)\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer,max_new_tokens=200)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline = pipe,\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 1000000},\n",
        ")"
      ],
      "metadata": {
        "id": "PdLe4Xka-cIB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4: Create the prompt template\n",
        "During inference, when for the query sent to the vector search, the vector DB will provide multiple documents, from which we choose the best. This becomes the context in the prompt template."
      ],
      "metadata": {
        "id": "m49OOl0aN0ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the 1st method: this is a simpler approach."
      ],
      "metadata": {
        "id": "B6m_IrSQItKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "T3XOYqVFMwtQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the 2nd method using ChatProptTemplate. This is a more sophasticated approach. More information on the difference between these 2 templates are mentioned [here](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)."
      ],
      "metadata": {
        "id": "jcWBmoUJI9-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "### CONTEXT\n",
        "{context}\n",
        "\n",
        "### QUESTION\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "k2xD_9b-G0nC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step5: Create a retriever\n",
        "We can use a RetrieverQA chain from langchain for this. See [this](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain)."
      ],
      "metadata": {
        "id": "rIOKPJwwQSSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\" : 3}),\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        "    )"
      ],
      "metadata": {
        "id": "0Uxua12nOU8d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a more sophasticated approach. This approach is explained better in [LangChain expression language notebook](https://colab.research.google.com/drive/1yFgTXd3sUa83-QWUDQWoyywduCIlT2E6#scrollTo=n34J771m_hh8&line=4&uniqifier=1)."
      ],
      "metadata": {
        "id": "WjeQVZVEJJ_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "base_retriever = db.as_retriever(search_kwargs={\"k\" : 3})\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "QnSqarPVG0Gz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain ({ \"query\" : question })\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZYZFZ3KXw_x",
        "outputId": "1c7ed38c-93dc-4ac3-dff6-8a0fdd8a20a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'How many credits does the thesis comprise of?', 'result': '30'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JTK7RDGE8Tw",
        "outputId": "2a1b6f6d-bdfc-48cc-d878-91da10b9a08b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'response': '30', 'context': [Document(page_content='• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0}), Document(page_content='programming, database management and big data  analytics.\\n \\n \\nA majority  of the master  theses  in the program  are written  in collaboration  with  the \\nindustry, based on a specific problem that the firm wants to solve. This letter sets \\nout the requirements for the thesis work as an aid for the firm. In particular, here \\nare some  important  things  to keep  in mind while  discussing  a potential  project:  \\n \\n• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0}), Document(page_content='applications should therefore be based on statistically oriented  \\nmethods.  \\n• The master  thesis  is a scientific  work . This  means  for example  that  \\nthe proposed  solution  in the thesis  needs  to relate  to existing work  in \\nthe scientific literature, and the advantages and disadvantages of \\nthe proposed  solution  needs  to be critically  assessed.  \\n• A master thesis is a public document  the results from the thesis work will', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 1})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF3-DeIRJR5F",
        "outputId": "d309e41e-442b-4338-82f0-2f496c514784"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQf1xmP6JgKM",
        "outputId": "b0b003b4-9fa7-4ce0-f3c8-d76a4a288161"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='programming, database management and big data  analytics.\\n \\n \\nA majority  of the master  theses  in the program  are written  in collaboration  with  the \\nindustry, based on a specific problem that the firm wants to solve. This letter sets \\nout the requirements for the thesis work as an aid for the firm. In particular, here \\nare some  important  things  to keep  in mind while  discussing  a potential  project:  \\n \\n• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNYJqmAbJkwj",
        "outputId": "ea9af527-4e38-4e0c-e55c-f4779ce92e84"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='applications should therefore be based on statistically oriented  \\nmethods.  \\n• The master  thesis  is a scientific  work . This  means  for example  that  \\nthe proposed  solution  in the thesis  needs  to relate  to existing work  in \\nthe scientific literature, and the advantages and disadvantages of \\nthe proposed  solution  needs  to be critically  assessed.  \\n• A master thesis is a public document  the results from the thesis work will', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step6: Evaluatiopn of the RAG pipeline\n",
        "We will evaluate using RAGAS( [paper](https://arxiv.org/abs/2309.15217))."
      ],
      "metadata": {
        "id": "YBm5JVFN6AOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "LG2-z4Yg9jLW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "eval_dataset = Dataset.from_csv(\"data/RagEvalGT.csv\", encoding='latin1', sep = ';')"
      ],
      "metadata": {
        "id": "0nLhZueK_MwI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_93B-lA_jPa",
        "outputId": "9044666d-1f66-4ab5-fd3f-e4b0bbe433e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'ground_truth'],\n",
              "    num_rows: 8\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset for ragas needs to be in a particular structure, see [page](https://docs.ragas.io/en/latest/howtos/applications/data_preparation.html).\n",
        "\n",
        "\n",
        "1. For the question, we load it from the question field from the csv.\n",
        "2. For the answer, we get the response from the generator.\n",
        "3. For the contexts, we get the contexts retrieved by the retriever.\n",
        "4. For the ground truths, we get the answer from the csv.\n",
        "\n"
      ],
      "metadata": {
        "id": "71VRanoAKDW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm -q"
      ],
      "metadata": {
        "id": "oFTzyCPmMudA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def create_ragas_dataset(rag_chain, eval_dataset):\n",
        "  rag_dataset = []\n",
        "  for row in tqdm(eval_dataset):\n",
        "    answer = rag_chain.invoke({\"question\" : row[\"question\"]})\n",
        "    rag_dataset.append(\n",
        "        {\"question\" : row[\"question\"],\n",
        "         \"answer\" : answer[\"response\"],\n",
        "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
        "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
        "         }\n",
        "    )\n",
        "  rag_df = pd.DataFrame(rag_dataset)\n",
        "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
        "  return rag_eval_dataset"
      ],
      "metadata": {
        "id": "SZWMI3USKU8z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_qa_ragas_dataset = create_ragas_dataset(retrieval_augmented_qa_chain, eval_dataset)\n",
        "basic_qa_ragas_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GYyIQ8POAkl",
        "outputId": "3cc38996-b53b-4d75-964c-22bcb9ea2b0a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [02:19<00:00, 17.48s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
              "    num_rows: 8\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basic_qa_ragas_dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncyMQ-PPPL-S",
        "outputId": "16e6d2f5-cd78-4475-cbf2-6e6f6be0aad5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'How many credit points does the thesis comprise of?',\n",
              " 'answer': '30',\n",
              " 'contexts': ['• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.',\n",
              "  'programming, database management and big data  analytics.\\n \\n \\nA majority  of the master  theses  in the program  are written  in collaboration  with  the \\nindustry, based on a specific problem that the firm wants to solve. This letter sets \\nout the requirements for the thesis work as an aid for the firm. In particular, here \\nare some  important  things  to keep  in mind while  discussing  a potential  project:  \\n \\n• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester',\n",
              "  'applications should therefore be based on statistically oriented  \\nmethods.  \\n• The master  thesis  is a scientific  work . This  means  for example  that  \\nthe proposed  solution  in the thesis  needs  to relate  to existing work  in \\nthe scientific literature, and the advantages and disadvantages of \\nthe proposed  solution  needs  to be critically  assessed.  \\n• A master thesis is a public document  the results from the thesis work will'],\n",
              " 'ground_truths': ['30']}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataset to a Parquet file\n",
        "save_path = '/content/basic_qa_ragas_dataset.parquet'\n",
        "basic_qa_ragas_dataset.to_pandas().to_parquet(save_path)"
      ],
      "metadata": {
        "id": "qWx5OjUWfo6e"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_qa_ragas_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxwn2PpwgeI2",
        "outputId": "56a5044d-0431-4b4e-85ed-c75d74c12331"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
              "    num_rows: 8\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "save_path = '/content/basic_qa_ragas_dataset.parquet'\n",
        "ragas_eval_dataset =  Dataset.from_pandas(pd.read_parquet(save_path))\n",
        "ragas_eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLZrsu8Fghh9",
        "outputId": "11878630-6728-45d7-c291-31a929871aa7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
              "    num_rows: 8\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have got the dataset in the structure expected by the RAGAS module, we can go ahead and evaluate our whole pipeline."
      ],
      "metadata": {
        "id": "A0rTB_HWP5tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U ragas"
      ],
      "metadata": {
        "id": "pC3zr4IJQfY-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAGAS evaluation needs OpenAI api key. Can be setup [here](https://platform.openai.com/api-keys). The usage can be tracked [here](https://platform.openai.com/usage)."
      ],
      "metadata": {
        "id": "4zBDtzZwWS5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "open_ai_key = getpass.getpass('Enter your OPENAI API Key')\n",
        "os.environ['OPENAI_API_KEY'] = open_ai_key"
      ],
      "metadata": {
        "id": "7Mjk8XXCWKfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c7d93f-77a6-42d4-f88a-4af40c51ab0e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OPENAI API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    context_relevancy,\n",
        "    answer_correctness,\n",
        "    answer_similarity\n",
        ")\n",
        "from ragas import evaluate\n",
        "eval_result = evaluate(\n",
        "  ragas_eval_dataset,\n",
        "  metrics=[\n",
        "      context_precision,\n",
        "      faithfulness,\n",
        "      answer_relevancy,\n",
        "      context_recall,\n",
        "      context_relevancy,\n",
        "      answer_correctness,\n",
        "      answer_similarity\n",
        "  ],\n",
        ")\n",
        "eval_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlV2MkRzQmer",
        "outputId": "ab1252a1-b1ce-4b80-8309-5058d9fd413f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [context_precision]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [faithfulness]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:14<00:00, 14.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [answer_relevancy]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [context_recall]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [context_relevancy]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [answer_correctness]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [answer_similarity]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context_precision': 0.6875, 'faithfulness': 0.7500, 'answer_relevancy': 0.7136, 'context_recall': 1.0000, 'context_relevancy': 0.0275, 'answer_correctness': 0.6130, 'answer_similarity': 0.8897}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "\n",
        "\n",
        "1.   While this works for simple pdf documents, complicated pdf documents resulted in very poor results.\n",
        "\n"
      ],
      "metadata": {
        "id": "dlAjRbET1ra5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "1.   [Post](https://www.linkedin.com/pulse/get-insight-from-your-business-data-build-llm-application-jain/) by Ashish Jain.\n",
        "2.   [Blog](https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed) by Onkar Mishra\n",
        "3. [Langchain course](https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/1/introduction) on DeepLearning.ai\n",
        "\n"
      ],
      "metadata": {
        "id": "euWvF0uE4CPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To-Do(if time is available)\n",
        "1. Checkout [Langchain Expression Language](https://www.youtube.com/watch?v=moJRxxEddzU)\n",
        "2. Checkout about Langchain chunking(RecursiveCharacterTextSplitter). A [lead](https://www.youtube.com/watch?v=eqOfr4AGLk8) maybe."
      ],
      "metadata": {
        "id": "dIKRgH-5QKjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps:\n",
        "1. Explain each metric reported in RAGAS. Fine-tune.\n",
        "2. Check what details are needed for the Harry Potter data. Both eval and pdf document to parse."
      ],
      "metadata": {
        "id": "_tPkT7YnC3xv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXJuiQem4MWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}