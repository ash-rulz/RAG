{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdXdy++5muCfyij9YzjUHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ash-rulz/RAG/blob/main/RAG_Lanchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG based QA using Langchain\n",
        "\n",
        "We are using FLAN_T5 model in this excercise. The predecessor of this model is the T5 model which originated from the paper - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(see [video](https://www.youtube.com/watch?v=91iLu6OOrwk) for more details on the paper, or even detailed video [here](https://www.youtube.com/watch?v=Axo0EtMUK90)). The T5 model is based on the paper Scaling Instruction-Finetuned Language Models(see [video](https://www.youtube.com/watch?v=SHMsdAPo2Ls)). FLAN T5 is just Fine-tuned LANguage model on T5.\n",
        "\n",
        "Next, we use [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) which converts the sentences to a 384 dimensional vector space. This sentence embedding is stored in FAISS vector DB.\n",
        "\n",
        "The relevant documents are retrieved using *RetrievalQA* from *langchain*."
      ],
      "metadata": {
        "id": "weC8tWvd8fnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites\n",
        "Create a folder called example_data and in that folder place the pdf to be stored in the vector database."
      ],
      "metadata": {
        "id": "_LMqN8d8IZz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Split the PDF"
      ],
      "metadata": {
        "id": "FhfM7KE2K1dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "!pip install pypdf -q"
      ],
      "metadata": {
        "id": "J-Ksi8wm3X63"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the pdf to memory\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "pdfLoader = PyPDFLoader(\"data/LetterToIndustry.pdf\")\n",
        "documents = pdfLoader.load()"
      ],
      "metadata": {
        "id": "FWqT6IM-5JBB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mflrq0mu5vLB",
        "outputId": "631442a6-ceb8-4945-92d9-98fb9402e597"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the file to chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "jVmzx9Bd53BB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inNZqPcp6VNP",
        "outputId": "838630cc-3f5d-47a0-94a4-9708715e696b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2: Create vector store\n",
        "Here we use all-MiniLM-L6-v2 to create the sentence embedding and the embeddings are stored in the [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss) vector store."
      ],
      "metadata": {
        "id": "00Hs7y-iLEnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q sentence-transformers"
      ],
      "metadata": {
        "id": "5RB3TlX87Opv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_path,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "iOugJauJ6cY6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-gpu"
      ],
      "metadata": {
        "id": "27T1vXPx-GOx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create vector store\n",
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "MnEg-wk87XO1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of documents retrieved from the vector DB for a question\n",
        "question = \"How many credits does the thesis comprise of?\"\n",
        "searchDocs = db.similarity_search_with_score(question)\n",
        "print(searchDocs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukQ8eKgn8Dub",
        "outputId": "1f4593f5-c3db-443e-949d-a10137be1f75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Document(page_content='• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0}), 0.6723114)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3: Create the generator\n",
        "Here we use the FLAN-T5 model which is fine-tuned on many tasks including QA tasks.\n"
      ],
      "metadata": {
        "id": "ZVsYnr_JMds_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "model_name_flan = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_flan)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_flan)\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer,max_new_tokens=200)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline = pipe,\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 1000000},\n",
        ")"
      ],
      "metadata": {
        "id": "PdLe4Xka-cIB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4: Create the prompt template\n",
        "During inference, when for the query sent to the vector search, the vector DB will provide multiple documents, from which we choose the best. This becomes the context in the prompt template."
      ],
      "metadata": {
        "id": "m49OOl0aN0ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the 1st method: this is a simpler approach."
      ],
      "metadata": {
        "id": "B6m_IrSQItKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "T3XOYqVFMwtQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the 2nd method using ChatProptTemplate. This is a more sophasticated approach."
      ],
      "metadata": {
        "id": "jcWBmoUJI9-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "### CONTEXT\n",
        "{context}\n",
        "\n",
        "### QUESTION\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "k2xD_9b-G0nC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step5: Create a retriever\n",
        "We can use a RetrieverQA chain from langchain for this. See [this](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain)."
      ],
      "metadata": {
        "id": "rIOKPJwwQSSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\" : 3}),\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        "    )"
      ],
      "metadata": {
        "id": "0Uxua12nOU8d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a more sophasticated approach:"
      ],
      "metadata": {
        "id": "WjeQVZVEJJ_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "base_retriever = db.as_retriever(search_kwargs={\"k\" : 3})\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "QnSqarPVG0Gz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain ({ \"query\" : question })\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZYZFZ3KXw_x",
        "outputId": "acba0603-d6b2-4559-cca5-bc80a61d02dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'How many credits does the thesis comprise of?', 'result': '30'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JTK7RDGE8Tw",
        "outputId": "e3eed2dd-725c-4628-814b-ea108112422f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'response': '30', 'context': [Document(page_content='• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0}), Document(page_content='programming, database management and big data  analytics.\\n \\n \\nA majority  of the master  theses  in the program  are written  in collaboration  with  the \\nindustry, based on a specific problem that the firm wants to solve. This letter sets \\nout the requirements for the thesis work as an aid for the firm. In particular, here \\nare some  important  things  to keep  in mind while  discussing  a potential  project:  \\n \\n• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0}), Document(page_content='applications should therefore be based on statistically oriented  \\nmethods.  \\n• The master  thesis  is a scientific  work . This  means  for example  that  \\nthe proposed  solution  in the thesis  needs  to relate  to existing work  in \\nthe scientific literature, and the advantages and disadvantages of \\nthe proposed  solution  needs  to be critically  assessed.  \\n• A master thesis is a public document  the results from the thesis work will', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 1})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF3-DeIRJR5F",
        "outputId": "2f961988-ed4d-4541-96cd-90e02d4cf3fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQf1xmP6JgKM",
        "outputId": "f57da179-b6c3-4fe2-b874-d649b384608d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='programming, database management and big data  analytics.\\n \\n \\nA majority  of the master  theses  in the program  are written  in collaboration  with  the \\nindustry, based on a specific problem that the firm wants to solve. This letter sets \\nout the requirements for the thesis work as an aid for the firm. In particular, here \\nare some  important  things  to keep  in mind while  discussing  a potential  project:  \\n \\n• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNYJqmAbJkwj",
        "outputId": "adb63c9d-407a-4801-eb91-07a8c075d78e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='applications should therefore be based on statistically oriented  \\nmethods.  \\n• The master  thesis  is a scientific  work . This  means  for example  that  \\nthe proposed  solution  in the thesis  needs  to relate  to existing work  in \\nthe scientific literature, and the advantages and disadvantages of \\nthe proposed  solution  needs  to be critically  assessed.  \\n• A master thesis is a public document  the results from the thesis work will', metadata={'source': 'data/LetterToIndustry.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step6: Evaluatiopn of the RAG pipeline\n",
        "We will evaluate using RAGAS( [paper](https://arxiv.org/abs/2309.15217))."
      ],
      "metadata": {
        "id": "YBm5JVFN6AOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "LG2-z4Yg9jLW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "eval_dataset = Dataset.from_csv(\"data/RagEvalGT.csv\", encoding='latin1', sep = ';')"
      ],
      "metadata": {
        "id": "0nLhZueK_MwI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_93B-lA_jPa",
        "outputId": "56a322a2-6258-46a7-a3b8-39198add97c6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'context', 'ground_truth'],\n",
              "    num_rows: 8\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset for ragas needs to be in a particular structure, see [page](https://docs.ragas.io/en/latest/howtos/applications/data_preparation.html).\n",
        "\n",
        "\n",
        "1. For the question, we load it from the question field from the csv.\n",
        "2. For the answer, we get the response from the generator.\n",
        "3. For the contexts, we get the contexts retrieved by the retriever.\n",
        "4. For the ground truths, we get the answer from the csv.\n",
        "\n"
      ],
      "metadata": {
        "id": "71VRanoAKDW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm -q"
      ],
      "metadata": {
        "id": "oFTzyCPmMudA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def create_ragas_dataset(rag_chain, eval_dataset):\n",
        "  rag_dataset = []\n",
        "  for row in tqdm(eval_dataset):\n",
        "    answer = rag_chain.invoke({\"question\" : row[\"question\"]})\n",
        "    rag_dataset.append(\n",
        "        {\"question\" : row[\"question\"],\n",
        "         \"answer\" : answer[\"response\"],\n",
        "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
        "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
        "         }\n",
        "    )\n",
        "  rag_df = pd.DataFrame(rag_dataset)\n",
        "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
        "  return rag_eval_dataset"
      ],
      "metadata": {
        "id": "SZWMI3USKU8z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_qa_ragas_dataset = create_ragas_dataset(retrieval_augmented_qa_chain, eval_dataset)\n",
        "basic_qa_ragas_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GYyIQ8POAkl",
        "outputId": "15841495-0624-4ef9-b8e5-340c8f81df4e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [01:11<00:00,  8.99s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
              "    num_rows: 8\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basic_qa_ragas_dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncyMQ-PPPL-S",
        "outputId": "75696643-b15b-4fc2-c59b-4f7b4d790181"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'How many credit points does the thesis comprise of?',\n",
              " 'answer': '30',\n",
              " 'contexts': ['• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester  \\n(the academic year in Sweden is divided in two semesters).  \\n• The thesis  is organized  as a course  and runs  between  fixed  dates  \\n(Jan - June  for the spring  semester  or Sept -Jan for the fall semester).  \\n• The thesis  is individual  work  and the student  has to write  a \\nsingle -authored master thesis report.  \\n• The proposed project needs to be accepted  by the course \\nexaminer.',\n",
              "  'programming, database management and big data  analytics.\\n \\n \\nA majority  of the master  theses  in the program  are written  in collaboration  with  the \\nindustry, based on a specific problem that the firm wants to solve. This letter sets \\nout the requirements for the thesis work as an aid for the firm. In particular, here \\nare some  important  things  to keep  in mind while  discussing  a potential  project:  \\n \\n• The thesis  comprises  30 credit  points,  i.e. 100%  studies  for a semester',\n",
              "  'applications should therefore be based on statistically oriented  \\nmethods.  \\n• The master  thesis  is a scientific  work . This  means  for example  that  \\nthe proposed  solution  in the thesis  needs  to relate  to existing work  in \\nthe scientific literature, and the advantages and disadvantages of \\nthe proposed  solution  needs  to be critically  assessed.  \\n• A master thesis is a public document  the results from the thesis work will'],\n",
              " 'ground_truths': ['30']}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have got the dataset in the structure expected by the RAGAS module, we can go ahead and evaluate our whole pipeline."
      ],
      "metadata": {
        "id": "A0rTB_HWP5tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragas -q"
      ],
      "metadata": {
        "id": "pC3zr4IJQfY-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAGAS evaluation needs OpenAI api key. Can be setup [here](https://platform.openai.com/api-keys). The usage can be tracked [here](https://platform.openai.com/usage)."
      ],
      "metadata": {
        "id": "4zBDtzZwWS5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "open_ai_key = getpass.getpass('Enter your OPENAI API Key')\n",
        "os.environ['OPENAI_API_KEY'] = open_ai_key"
      ],
      "metadata": {
        "id": "7Mjk8XXCWKfv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    context_relevancy,\n",
        "    answer_correctness,\n",
        "    answer_similarity\n",
        ")\n",
        "from ragas import evaluate\n",
        "eval_result = evaluate(\n",
        "  basic_qa_ragas_dataset,\n",
        "  metrics=[\n",
        "      context_precision,\n",
        "      faithfulness,\n",
        "      answer_relevancy,\n",
        "      context_recall,\n",
        "      context_relevancy,\n",
        "      answer_correctness,\n",
        "      answer_similarity\n",
        "  ],\n",
        ")\n",
        "eval_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "BlV2MkRzQmer",
        "outputId": "f6432e64-a442-4c52-f98b-4f827b26075c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [context_precision]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:24<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/async_utils.py\u001b[0m in \u001b[0;36mrun_async_tasks\u001b[0;34m(tasks, show_progress, progress_bar_desc)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/async_utils.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks_to_execute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\u001b[0m in \u001b[0;36masync_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masync_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/llms/openai.py\u001b[0m in \u001b[0;36magenerate\u001b[0;34m(self, prompt, n, temperature, callbacks)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# TODO: use callbacks for llm generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         completion = await self._client.chat.completions.create(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1295\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         )\n\u001b[0;32m-> 1536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     ) -> ResponseT | _AsyncStreamT:\n\u001b[0;32m-> 1315\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                 return await self._retry_request(\n\u001b[0m\u001b[1;32m   1379\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1419\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                 return await self._retry_request(\n\u001b[0m\u001b[1;32m   1379\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1419\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-e4004f1c8de0>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m eval_result = evaluate(\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mbasic_qa_ragas_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   metrics=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, column_map)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mbinary_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"evaluating with [{metric.name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# log the evaluation event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, dataset, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtrace_as_chain_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ragas_{self.name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\u001b[0m in \u001b[0;36m_score_batch\u001b[0;34m(self, dataset, callbacks, callback_group_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             results = self.llm.generate(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/llms/openai.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, n, temperature, callbacks)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbacks\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     ) -> t.Any:  # TODO: LLMResult\n\u001b[0;32m--> 153\u001b[0;31m         llm_results = run_async_tasks(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/async_utils.py\u001b[0m in \u001b[0;36mrun_async_tasks\u001b[0;34m(tasks, show_progress, progress_bar_desc)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# may occur in some environments where tqdm.asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fatal error occurred while running async tasks.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ('Fatal error occurred while running async tasks.', RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "\n",
        "\n",
        "1.   While this works for simple pdf documents, complicated pdf documents resulted in very poor results.\n",
        "\n"
      ],
      "metadata": {
        "id": "dlAjRbET1ra5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "1.   [Post](https://www.linkedin.com/pulse/get-insight-from-your-business-data-build-llm-application-jain/) by Ashish Jain.\n",
        "2.   [Blog](https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed) by Onkar Mishra\n",
        "3. [Langchain course](https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/1/introduction) on DeepLearning.ai\n",
        "\n"
      ],
      "metadata": {
        "id": "euWvF0uE4CPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "1. Check the advanced lanchain prompting technique used."
      ],
      "metadata": {
        "id": "dIKRgH-5QKjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXJuiQem4MWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}